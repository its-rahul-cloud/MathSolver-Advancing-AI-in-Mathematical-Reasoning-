{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8330401,"sourceType":"datasetVersion","datasetId":4946449},{"sourceId":8449074,"sourceType":"datasetVersion","datasetId":5034873},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/","metadata":{"execution":{"iopub.status.busy":"2024-06-21T08:03:23.504590Z","iopub.execute_input":"2024-06-21T08:03:23.505575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The work in this notebook is inspired by these notebooks:\n* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"from threading import Thread\nimport gc\nimport os\nimport io\nimport json\nimport random\nimport pickle\nimport zipfile\nimport datetime\nimport time\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom torch.cuda.amp import autocast\nfrom IPython.display import display\nimport torch.nn.functional as F\nimport tokenizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nMODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\nWEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\nMAX_LENGTH = 1284\nBATCH_SIZE = 8\nDEVICE = torch.device(\"cuda\")    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Data ","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ndisplay(sample_sub)\ndisplay(test.head(5))\n\n# Prepare text for model\ntest['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\nprint(test['text'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n\ntokens = tokenizer(test['text'].tolist(), padding='max_length',\n                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n\nINPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\nATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n\n# Move tensors to CPU and convert them to lists\ninput_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\nattention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = input_ids_cpu\ndata['ATTENTION_MASKS'] = attention_masks_cpu\ndata[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model \n> We load 1 model on each gpu.  ","metadata":{}},{"cell_type":"code","source":"# BitsAndBytes configuration\nbnb_config =  BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16,\n    bnb_8bit_use_double_quant=False)\n\n# Load base model on GPU 0\ndevice0 = torch.device('cuda:0')\n\nbase_model_0 = LlamaForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:0')\nbase_model_0.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load base model on GPU 1\ndevice1 = torch.device('cuda:1')\nbase_model_1 = LlamaForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:1')\nbase_model_1.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load weights \n","metadata":{}},{"cell_type":"code","source":"# LoRa configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.10,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get peft\nmodel_0 = get_peft_model(base_model_0, peft_config).to(device0) \n#Load weights\nmodel_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_0.eval()\n\nmodel_1 = get_peft_model(base_model_1, peft_config).to(device1)\nmodel_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_1.eval()\n\n#Trainable Parameters\nmodel_0.print_trainable_parameters(), model_1.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def inference(df, model, device, batch_size=BATCH_SIZE):\n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n    \n    generated_class_a = []\n    generated_class_b = []\n    generated_class_c = []\n\n    model.eval()\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        \n        with torch.no_grad():\n            with autocast():\n                outputs = model(\n                    input_ids=batch_input_ids,\n                    attention_mask=batch_attention_mask\n                )\n        \n        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        generated_class_a.extend(probabilities[:, 0])\n        generated_class_b.extend(probabilities[:, 1])\n        generated_class_c.extend(probabilities[:, 2])\n    \n    df['winner_model_a'] = generated_class_a\n    df['winner_model_b'] = generated_class_b\n    df['winner_tie'] = generated_class_c\n\n    torch.cuda.empty_cache()  \n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\n\nN_SAMPLES = len(data)\n\n# Split the data into two subsets\n# half = round(N_SAMPLES / 2)\n\nhalf = 15000\nsub1 = data.iloc[0:half].copy()\nsub2 = data.iloc[half:N_SAMPLES].copy()\n\n# Function to run inference in a thread\ndef run_inference(df, model, device, results, index):\n    results[index] = inference(df, model, device)\n\n# Dictionary to store results from threads\nresults = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start threads\nt0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\nt1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n\nt0.start()\nt1.start()\n\n# Wait for all threads to finish\nt0.join()\nt1.join()\n\n# Combine results back into the original DataFrame\ndata = pd.concat([results[0], results[1]], axis=0)\n\nprint(f\"Processing complete. Total time: {time.time() - st}\")\n\nTARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n\nsample_sub[TARGETS] = data[TARGETS]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llama_preds = data[TARGETS].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM + tfidf","metadata":{}},{"cell_type":"code","source":"TAG = 'lmsys-chatbot-arena'\nRUNPOD = os.path.exists('/workspace/')\nKAGGLE = not RUNPOD\nif KAGGLE: \n    print('kaggle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import pandas as pd\nexcept:\n    !pip install -q kaggle\n    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n    !pip install -q protobuf \n    !pip install -q numba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA = '/data/' if RUNPOD else 'data/' \\\n        if not os.path.exists('/kaggle/') \\\n            else '/kaggle/input/{}/'.format(TAG)\n\nif RUNPOD:\n    if not os.path.exists('~/.kaggle/kaggle.json'):\n        !mkdir -p ~/.kaggle\n        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json\n        !chmod 600 /root/.kaggle/kaggle.json\n\n    if not os.path.exists('/workspace/' + TAG + '.zip'):\n        !kaggle competitions download $TAG -p /workspace/ \n        \n    if not os.path.exists('/data/'):\n        import zipfile\n        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '/kaggle/input/'  \nMODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\nMODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'\nprint(MODEL_PATH)\n\nCODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'\nSAVE_PATH = MODEL_PATH if not KAGGLE else ''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(open(DATA + 'train.csv', 'r'))\ntest = pd.read_csv(open(DATA + 'test.csv', 'r'))\nsample = pd.read_csv(DATA + 'sample_submission.csv')\nprint(len(train), len(test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {}\nif False: \n    pass;\n    params['subsample'] = 30\nelse:\n    params['fold'] = -1\n\n\nparams['n_epochs'] = 1\nparams['n_lgb'] = 1\nparams['model'] = 'microsoft/deberta-v3-small'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params = {}\nFULL = params.get('fold', 0) < 0\nN_FOLDS = int(params.get('n_folds', 3)); \nFOLD = int(params.get('fold', 0))\nSEED = int(params.get('seed', 3))\nSS = int(params.get('subsample', 1))\n\nprint(N_FOLDS, FOLD, SEED, SS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\ndef get_folds(train): \n    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))\n\ntrain_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]\nif SS > 1:\n    train_ids, test_ids = train_ids[::SS], test_ids[::SS]\n\nprint(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(datetime.datetime.now().microsecond)\nrandom.seed(datetime.datetime.now().microsecond)\nnp.random.seed(datetime.datetime.now().microsecond)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN = False\nINFER = True \nSAVE = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGB = True\nTRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0\nINFER_LGB = not TRAIN and LGB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))\nccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def symlog(x):\n    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n\ndef dense(x):\n    x = np.asarray(x.astype(np.float32).todense())\n    x = symlog(x)\n    return x\n\ndef get_features(df):\n    pfeat = np.hstack([dense(v.transform(df[c])) \n                for v in [cvec, ccvec]\n                    for c in ['prompt', ]])\n    afeat = np.hstack([dense(v.transform(df[c])) \n                for c in ['response_a', ]\n                    for v in [cvec, ccvec]\n                ])\n    bfeat = np.hstack([dense(v.transform(df[c])) \n                for c in ['response_b', ]\n                    for v in [cvec, ccvec]\n                ])\n    \n    v = np.hstack([\n          afeat - bfeat, np.abs(afeat - bfeat), \n        ])\n    try: \n        v = v / (len(all_vote_models) if len(df) < len(train) else 1)\n    except:\n        pass\n\n    extras = []\n    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n    for e in EXTRAS:\n        for c in ['prompt', 'response_a', 'response_b']:\n            extras.append(df[c].str.count(e).values)\n            \n    extras.append(df[c].str.len())\n    extras.append(df[c].str.split().apply(lambda x: len(x)))\n    \n    extras = np.stack(extras, axis = 1)\n    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n    return np.hstack([v, extras])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if INFER and params.get('n_lgb', 1) > 0:\n    df = test\n    yps = []; b = 1000\n    for i in range(0, len(df), b):\n        arr = get_features(df.iloc[i: i + b])\n        ypms = []\n        for model in lgb_models:\n            ypms.append(model.predict_proba(arr))\n        yps.append(np.stack(ypms).mean(0))\n        print('.', end = '')\n        \n        if len(yps) % 2 == 0:\n            gc.collect()\n    print()\n\n    yp = np.concatenate(yps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_preds = yp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blend predictions\n\n$\\operatorname{preds} = 0.12 \\cdot \\operatorname{lgbm boosting preds} + 0.8 \\cdot \\operatorname{llama preds}$\n","metadata":{}},{"cell_type":"code","source":"lgb_wt = 0.2\npreds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])\ndisplay(out.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}